{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding grads.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gan4x4/CV-HSE2019/blob/master/train/Understanding_grads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX2Ou8yMXmNM",
        "colab_type": "code",
        "outputId": "f938e0f1-82a2-46b8-e7eb-1ea0bf484da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2, 1.2, 0, 4],requires_grad= True)\n",
        "\n",
        "y = x*x\n",
        "z = y.sum()\n",
        "\n",
        "print (\"X =\",x)\n",
        "print (\"X.grad = \",x.grad) # grad is none\n",
        "\n",
        "z.backward() \n",
        "print (\"y.grad = \", y.grad)\n",
        "print (\"x.grad = \", x.grad)\n",
        "\n",
        "# loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
        "# These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
        "# x.grad += dloss/dx\n",
        "#\n",
        "# z = loss\n",
        "# We want dz/dx\n",
        "#\n",
        "# dz/dx = dz/dy * dy/dx\n",
        "# y = y0 + y1 + y2 + y3\n",
        "# dz/dy = [1,1,1,1]\n",
        "#\n",
        "# y = x^2\n",
        "# dy/dx = 2x "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n",
            "X.grad =  None\n",
            "y.grad =  None\n",
            "x.grad =  tensor([4.0000, 2.4000, 0.0000, 8.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmxoVXnRbEn7",
        "colab_type": "code",
        "outputId": "bdde8e5f-17a1-4166-ac91-3b1855b5550c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "z.backward() # Error on second backward call"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9cee8d9ef07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVLOs_lMe1It",
        "colab_type": "code",
        "outputId": "e4ef5f0f-053a-4d4b-f279-4a75c8d7e279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "x = torch.tensor([2,1.2,0,4],requires_grad= True)\n",
        "print (\"X = \",x)\n",
        "\n",
        "y = x*x\n",
        "\n",
        "# Now grads for intermediate tensor y stay in memory\n",
        "y.retain_grad()\n",
        "\n",
        "z = y.sum()\n",
        "print (\"z (loss) =\",z)\n",
        "print (\"y =\",y)\n",
        "print (\"x =\",x)\n",
        "\n",
        "print (\"dz/dx \",x.grad) # grad is None\n",
        "\n",
        "print(\"========== Backprop 1 ==============\")\n",
        "z.backward(retain_graph=True) \n",
        "print (\"dz/dy \",y.grad)\n",
        "print (\"dz/dx \",x.grad)\n",
        "print(\"========== Backprop 2 ==============\")\n",
        "z.backward() \n",
        "\n",
        "# Grads are accumulated\n",
        "print (\"dz/dy \",y.grad) \n",
        "print (\"dz/dx \",x.grad)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X =  tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n",
            "z (loss) = tensor(21.4400, grad_fn=<SumBackward0>)\n",
            "y = tensor([ 4.0000,  1.4400,  0.0000, 16.0000], grad_fn=<MulBackward0>)\n",
            "x = tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n",
            "dz/dx  None\n",
            "========== Backprop 1 ==============\n",
            "dz/dy  tensor([1., 1., 1., 1.])\n",
            "dz/dx  tensor([4.0000, 2.4000, 0.0000, 8.0000])\n",
            "========== Backprop 2 ==============\n",
            "dz/dy  tensor([2., 2., 2., 2.])\n",
            "dz/dx  tensor([ 8.0000,  4.8000,  0.0000, 16.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HICqFWVgQ4D",
        "colab_type": "code",
        "outputId": "c1a131aa-2575-40dd-cf6c-b27d4402baf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "x = torch.tensor([2,1.2,0,4],requires_grad= True)\n",
        "print (\"X = \",x)\n",
        "\n",
        "y = x*x\n",
        "z = y.sum()\n",
        "\n",
        "print (\"dz/dy \",y.grad) \n",
        "print (\"dz/dx \",x.grad)\n",
        "\n",
        "optimizer = optim.SGD([x],lr=0.1, momentum=0.9)\n",
        "z.backward()\n",
        "print(\"========== Backprop 1 ==============\")\n",
        "print (\"dz/dy \",y.grad) \n",
        "print (\"dz/dx \",x.grad)\n",
        "print(\"========== Optimize ==============\")\n",
        "optimizer.step()\n",
        "print (\"X = \",x)\n",
        "print(\"========== Zero grad ==============\")\n",
        "optimizer.zero_grad() \n",
        "print (\"dz/dy \",y.grad) \n",
        "print (\"dz/dx \",x.grad)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X =  tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n",
            "dz/dy  None\n",
            "dz/dx  None\n",
            "dz/dy  None\n",
            "dz/dx  tensor([4.0000, 2.4000, 0.0000, 8.0000])\n",
            "X =  tensor([1.6000, 0.9600, 0.0000, 3.2000], requires_grad=True)\n",
            "dz/dy  None\n",
            "dz/dx  tensor([0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}